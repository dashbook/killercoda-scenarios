## Extract & Load (EL)

Now that we have everything setup, we can start with the actual tutorial. In
most cases, data systems are composed of a transactional and an analytical part.
Transactional systems read and write single entries while analytical
systems answer aggregate queries on multiple entries. In this tutorial, the
postgres database, that we've setup, plays the role of the transactional system
and the lakehouse will play the role of the analytical system. We will create
the lakehouse by applying the Extract-Load-Transform (ELT) paradigm. By doing
so, we will first copy the data from the transactional system to the analytical
system without applying any transformations. This is called the EL step, which
we will do next.

### Configure Airbyte Source and Destination

Dashtool handles the EL step by leveraging the [Airbyte](https://airbyte.com)
specification. The Airbyte specification defines a standard way to communicate
between data sources and destinations.

Let's define a Airbyte Source to extract the data from the Postgres database and a
Airbyte Destination to load it in to an Iceberg table. 
We'll define the Airbyte source and destination in the `bronze/inventory/postgres.ingest.json` file.
The `image` field specifies which docker container to use for the extraction.

#### Source

The `source` field contains the configuration parameters for the
[Postgres Source](https://github.com/dashbook/airbyte/blob/master/docs/integrations/sources/postgres.md).
It contains information about the connection, which schemas to extract and what
kind of replication to use. One great thing about the Postgres Source
is that it allows a log based replication which enables incremental extraction
of the data without difficult setup.

Run the following commands to create a logical replication slot and publication for the source:
```bash
kubectl  exec -ti postgres-0 -- env PGPASSWORD=postgres psql -h postgres -U postgres postgres -c "SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');"

kubectl  exec -ti postgres-0 -- env PGPASSWORD=postgres psql -h postgres -U postgres postgres -c "CREATE PUBLICATION airbyte_publication FOR TABLE inventory.orders, inventory.customers, inventory.products;"
```{{exec}}

#### Destination

The `destination` field contains configuration parameters for the
[Iceberg Destination](https://github.com/dashbook/destination-iceberg). It contains
information about which tables to extract, which iceberg catalog to use and
parameters for the S3 object store.

Dashtool creates the entities in the lakehouse according to the local git repository.
If the files exist on a branch in the git repository, it will create the same branch for the entity.
So to create the Iceberg tables for the Source and Destination we have to add the `postgres.ingest.json` file to a git branch.
For that we create a git repository and create a bronze branch.

```
git init
git add dashtool.json
git commit -m "initial commit"
git branch -M master main
git branch bronze
git checkout bronze
```{{exec}}

Let's add the the `prostgres.ingest.json` file to the bronze branch so that dashtool can create the corresponding tables.

```
git add bronze/inventory/postgres.ingest.json
git commit -m "bronze"
```{{exec}}

### Dashtool build

The build command creates a graph of all the entities we have created. So far we
only defined tables for ingestion, but we will soon ad more complex
transformations. One important thing to note is that the build command takes the
current git branch into account. Since Iceberg Tables support branching,
dashtool will create the table with the corresponding branch. Now we can run the dashtool build command:

```
./dashtool build
```{{exec}}

### Dashtool workflow

The workflow command takes the graph, generated by the build command, and
creates an Argo Workflow that can be used to update the data in the tables. The
output of the command is written to the `argo/workflow.yaml` file.

```
./dashtool workflow
```{{exec}}

### Create argo workflow

By executing the following command you deploy the created Workflow to the
Kubernetes cluster. Be default the workflow is defined as a "Cron" workflow that
will execute daily. If you want to change this, you can edit the
`argo/workflow.yaml` file.

```
kubectl apply -f argo/workflow.yaml
```{{exec}}

### Run Argo Workflow

Navigate your browser to the [Argo console]({{TRAFFIC_HOST1_32746}}) to access the Argo Workflow
UI. As mentioned earlier, you might see a warning from the browser that the page
uses a self-signed certificate, which is okay for our use case.

Go to the "Cron Workflows" tab on the left and select the "dashtool" workflow (you might need to remove any search filters to see it).
By pressing "Submit", the workflow will start and you will see information about
the individual steps.

### Merge changes into main

If your workflows ran successfully, you can merge the changes into the main
branch. This will also merge the "bronze" branch of the Iceberg tables into the "main" branch the next time you run dashtool.

```
git checkout main
git merge bronze
```{{exec}}

Run "dashtool build" again to merge the bronze tables onto the main branch so that they can be accessed from other processes.

```
./dashtool build
```{{exec}}
