## Extract & Load (EL)

Now that we have everything setup, we can start with the actual tutorial. In
most cases, data systems are composed of a transactional and an analytical part.
Transactional systems read and write single entries while analytical
systems answer aggregate queries on multiple entries. In this tutorial, the
postgres database, that we've setup, plays the role of the transactional system
and the lakehouse will play the role of the analytical system. We will create
the lakehouse by applying the Extract-Load-Transform (ELT) paradigm. By doing
so, we will first copy the data from the transactional system to the analytical
system without applying any transformations. This is called the EL step, which
we will do next.

### Configure Singer Tap and Target

Dashtool handles the EL step by leveraging the [Singer](www.singer.io)
specification. The Singer specification defines a standard way to communicate
between data sources and destinations, called Taps and Targets.

Let's define a Singer Tap to extract the data from the Postgres database and a
Singer Target to load it in to an Iceberg table. For that we create a git repository and add a bronze branch to it.

```
git init
git branch -M master main
git branch bronze
git checkout bronze
```{{exec}}

You will see the `tap.json` and `target.json` files in the `bronze/inventory`
folder.

#### Tap

The `tap.json` file contains the configuration parameters for the
[Pipelinewise Postgres Tap](https://github.com/transferwise/pipelinewise-tap-postgres).
It contains information about the connection, which schemas to extract and what
kind of replication to use. One great thing about the Pipelinewise Postgres Tap
is that it allows a log based replication which enables incremental extraction
of the data without difficult setup.

#### Target

The `target.json` file contains configuration parameters for the
[Iceberg Target](https://github.com/dashbook/target-iceberg). It contains
information about which tables to extract, which iceberg catalog to use and
parameters for the S3 object store.

### Dashtool build

The build command creates a graph of all the entities we have created. So far we
only defined tables for ingestion, but we will soon ad more complex
transformations. One important thing to note is that the build command takes the
current git branch into account. Since Iceberg Tables support branching,
dashtool will create the table with the corresponding branch. Let's add the the `tap.json` and `target.json` file to the bronze branch so that dashtool will create the corresponding tables.

```
git add bronze/inventory/tap.json bronze/inventory/target.json
```{{exec}}

Now we can run the dashtool build command:

```
./dashtool build
```{{exec}}

### Dashtool workflow

The workflow command takes the graph, generated by the build command, and
creates an Argo Workflow that can be used to update the data in the tables. The
output of the command is written to the `argo/workflow.yaml` file.

```
./dashtool workflow
```{{exec}}

### Create argo workflow

By executing the following command you deploy the created Workflow to the
Kubernetes cluster. Be default the workflow is defined as a "Cron" workflow that
will execute daily. If you want to change this, you can edit the
`argo/workflow.yaml` file.

```
kubectl apply -f argo/workflow.yaml
```{{exec}}

### Run Argo Workflow

Navigate your browser to the [Argo console]({{TRAFFIC_HOST1_2746}}) to access the Argo Workflow
UI. As mentioned earlier, you might see a warning from the browser that the page
uses a self-signed certificate, which is okay for our use case.

Go to the "Cron Workflows" tab on the left and select the "dashtool" workflow.
By pressing "run", the workflow will start and you will see information about
the individual steps.

### Merge changes into main

If your workflows ran successfully, you can merge the changes into the main
branch.

```
git checkout main
git merge bronze
```{{exec}}
