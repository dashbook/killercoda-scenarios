## Extract & Load (EL)

We will create the lakehouse by applying the Extract-Load-Transform (ELT) paradigm. By doing
so, we will first copy the data from the operational systems to the analytical
system without applying any transformations. This is called the EL step, which
we will do next.

### Configure Aribyte Source and Destination

Dashtool handles the EL step by leveraging the [Airbyte](https://airbyte.com)
specification. The Airbyte specification defines a standard way to communicate
between data sources and destinations.

Let's define Airbyte Sources to extract the data from the Kafka server and Postgres database and Airbyte Destinations to load it into Iceberg tables. 

#### Kafka Source & Destination

We'll define the Kafka source and destination in the `bronze/inventory/kafka.ingest.json` file.
The `image` field specifies which docker container to use for the extraction.

The `source` field contains configuration parameters for the [Kafka Source](https://github.com/dashbook/airbyte/blob/master/docs/integrations/sources/kafka.md).
It contains information about the connection, which topic wo extract and the schema for the topic.

The `destination` field contains configuration parameters for the
[Iceberg Destination](https://github.com/dashbook/destination-iceberg). It contains
information about which tables to extract, which iceberg catalog to use and
parameters for the S3 object store.

#### Postgres Source & Destination

The Postgres source and destination are defined in the `bronze/inventory/postgres.ingest.json` file.
The `source` field contains the configuration parameters for the
[Postgres Source](https://github.com/dashbook/airbyte/blob/master/docs/integrations/sources/postgres.md).
It contains information about the connection, which schemas to extract and what
kind of replication to use. One great thing about the Postgres Source
is that it allows a log based replication which enables incremental extraction
of the data without difficult setup.

Run the following commands to create a logical replication slot and publication for the source:
```bash
kubectl  exec -ti postgres-0 -- env PGPASSWORD=postgres psql -h postgres -U postgres postgres -c "SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');"

kubectl  exec -ti postgres-0 -- env PGPASSWORD=postgres psql -h postgres -U postgres postgres -c "CREATE PUBLICATION airbyte_publication FOR TABLE inventory.orders, inventory.customers, inventory.products;"
```{{exec}}

The `destination` field contains configuration parameters for the
[Iceberg Destination](https://github.com/dashbook/destination-iceberg). It contains
information about which tables to extract, which iceberg catalog to use and
parameters for the S3 object store.

#### Git repository

Dashtool creates the entities in the lakehouse according to the local git repository.
If the files exist on a branch in the git repository, it will create the same branch for the entity.
So to create the Iceberg tables for the Sources and Destinations we have to add the `kafka.ingest.json` and `postgres.ingest.json` files to a git branch.
For that we create a git repository and create a bronze branch.

```
git init
git add dashtool.json
git commit -m "initial commit"
git branch -M master main
git branch bronze
git checkout bronze
```{{exec}}

Let's add the the `kafka.ingest.json` and `postgres.ingest.json` files to the bronze branch so that dashtool can create the corresponding tables.

```
git add bronze/inventory/kafka.ingest.json bronze/inventory/postgres.ingest.json
git commit -m "bronze"
```{{exec}}

### Dashtool build

The build command creates a graph of all the entities we have created. So far we
only defined tables for ingestion, but we will soon ad more complex
transformations. One important thing to note is that the build command takes the
current git branch into account. Since Iceberg Tables support branching,
dashtool will create the table with the corresponding branch. Now we can run the dashtool build command:

```
./dashtool build
```{{exec}}

### Dashtool workflow

The workflow command takes the graph, generated by the build command, and
creates an Argo Workflow that can be used to update the data in the tables. The
output of the command is written to the `argo/workflow.yaml` file.

```
./dashtool workflow
```{{exec}}

### Create argo workflow

By executing the following command you deploy the created Workflow to the
Kubernetes cluster. Be default the workflow is defined as a "Cron" workflow that
will execute daily. If you want to change this, you can edit the
`argo/workflow.yaml` file.

```
kubectl apply -f argo/workflow.yaml
```{{exec}}

### Run Argo Workflow

Navigate your browser to the [Argo console]({{TRAFFIC_HOST1_32746}}) to access the Argo Workflow
UI. As mentioned earlier, you might see a warning from the browser that the page
uses a self-signed certificate, which is okay for our use case.

Go to the "Cron Workflows" tab on the left and select the "dashtool" workflow (you might need to remove any search filters to see it).
By pressing "Submit", the workflow will start and you will see information about
the individual steps.

### Merge changes into main

If your workflows ran successfully, you can merge the changes into the main
branch. This will also merge the "bronze" branch of the Iceberg tables into the "main" branch the next time you run dashtool.

```
git checkout main
git merge bronze
```{{exec}}

Run "dashtool build" again to merge the bronze tables onto the main branch so that they can be accessed from other processes.

```
./dashtool build
```{{exec}}
